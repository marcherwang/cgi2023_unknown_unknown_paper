{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UHHdQrdzhkHQ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from shutil import copy\n",
        "import random\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "from torch.nn import functional as F\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "\n",
        "import torchvision\n",
        "from torchvision import transforms, datasets\n",
        "from torchvision.datasets import ImageFolder\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "import shutil\n",
        "\n",
        "import random\n",
        "from sklearn.metrics import classification_report\n",
        "import torchvision.datasets as datasets\n",
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import ImageDraw\n",
        "import pandas as pd\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EDazXWjNDvpx"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_curve, auc, precision_recall_curve,f1_score, precision_score,recall_score"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Construct and train the models:"
      ],
      "metadata": {
        "id": "Nf-7fHAsmGRd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6oyX1wnfYrfU"
      },
      "outputs": [],
      "source": [
        "## VGG model\n",
        "\n",
        "class VGG(nn.Module):\n",
        "    def __init__(self, features, num_classes=1000, init_weights=False):\n",
        "        super(VGG, self).__init__()\n",
        "        self.features = features\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((7, 7))  # add an adaptive average pooling layer\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(p=0.5),\n",
        "            nn.Linear(512*7*7, 4096),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout(p=0.5),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(4096, num_classes)\n",
        "        )\n",
        "        if init_weights:\n",
        "            self._initialize_weights()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # N x 3 x 224 x 224\n",
        "        x = self.features(x)\n",
        "        # N x 512 x 7 x 7\n",
        "        x = self.avgpool(x)  #perform adaptive averaging pooling\n",
        "        # N x 512 x 1 x 1\n",
        "        x = x.view(x.size(0), -1)\n",
        "        # N x 512*7*7\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                # He initialization\n",
        "                nn.init.kaiming_uniform_(m.weight, mode='fan_in', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                # He initialization\n",
        "                nn.init.kaiming_uniform_(m.weight, mode='fan_in', nonlinearity='relu')\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "\n",
        "def make_features(cfg: list):\n",
        "    layers = []\n",
        "    in_channels = 3\n",
        "    for v in cfg:\n",
        "        if v == \"M\":\n",
        "            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
        "        else:\n",
        "            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n",
        "            layers += [conv2d, nn.ReLU(True)]\n",
        "            in_channels = v\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "cfgs = {\n",
        "    'vgg11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
        "    'vgg13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
        "    'vgg16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
        "    'vgg19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
        "}\n",
        "\n",
        "\n",
        "def vgg(model_name=\"vgg16\", **kwargs):\n",
        "    try:\n",
        "        cfg = cfgs[model_name]\n",
        "    except:\n",
        "        print(\"Warning: model number {} not in cfgs dict!\".format(model_name))\n",
        "        exit(-1)\n",
        "    model = VGG(make_features(cfg), **kwargs)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## ResNet model\n",
        "\n",
        "class ResBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super(ResBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(out_channels)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_channels = 64\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        nn.init.kaiming_normal_(self.conv1.weight, mode='fan_out', nonlinearity='relu')\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.layer1 = self._make_layer(64, 2, stride=1)\n",
        "        self.layer2 = self._make_layer(128, 2, stride=2)\n",
        "        self.layer3 = self._make_layer(256, 2, stride=2)\n",
        "        self.layer4 = self._make_layer(512, 2, stride=2)\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1,1))\n",
        "        self.fc = nn.Linear(512, num_classes)\n",
        "\n",
        "    def _make_layer(self, out_channels, blocks, stride):\n",
        "        strides = [stride] + [1]*(blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(ResBlock(self.in_channels, out_channels, stride))\n",
        "            self.in_channels = out_channels\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = self.avgpool(out)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.fc(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "q5IwExsrig9q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = ResNet(num_classes=2)\n",
        "print(model)"
      ],
      "metadata": {
        "id": "6TQgpWSRimue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z7ohbKIWb9eT"
      },
      "outputs": [],
      "source": [
        "net = vgg()\n",
        "print(net)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yMUq2qr75kp7"
      },
      "outputs": [],
      "source": [
        "## Function for training\n",
        "\n",
        "def train(model_name):\n",
        "    global train_losses, train_accuracies, val_losses, val_accuracies, val_f1_scores, val_precision_scores, val_recall_scores\n",
        "    global fpr, tpr, roc_auc, precision, recall\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(\"Using {} device.\".format(device))\n",
        "\n",
        "    data_transform = {\n",
        "        \"train\": transforms.Compose([transforms.RandomResizedCrop(224),\n",
        "                                     transforms.RandomHorizontalFlip(),\n",
        "                                     transforms.ToTensor(),\n",
        "                                     transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]),\n",
        "        \"val\": transforms.Compose([transforms.Resize((224, 224)),\n",
        "                                   transforms.ToTensor(),\n",
        "                                   transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
        "    }\n",
        "\n",
        "    image_path = '/content/drive/MyDrive/Deepfashion_data/outdata'\n",
        "    assert os.path.exists(image_path), \"{} path does not exist.\".format(image_path)\n",
        "\n",
        "    train_dataset = datasets.ImageFolder(root=os.path.join(image_path, \"train\"), transform=data_transform[\"train\"])\n",
        "    train_num = len(train_dataset)\n",
        "\n",
        "    class_list = train_dataset.class_to_idx\n",
        "    class_dict = dict((val, key) for key, val in class_list.items())\n",
        "    json_str = json.dumps(class_dict, indent=4)\n",
        "    with open('/content/drive/MyDrive/model/class_indices.json', 'w') as json_file:\n",
        "        json_file.write(json_str)\n",
        "\n",
        "    batch_size = 32\n",
        "    nw = min([os.cpu_count(), batch_size if batch_size > 1 else 0, 8])\n",
        "    print('Using {} dataloader workers every process'.format(nw))\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=nw)\n",
        "\n",
        "    validate_dataset = datasets.ImageFolder(root=os.path.join(image_path, \"val\"), transform=data_transform[\"val\"])\n",
        "    val_num = len(validate_dataset)\n",
        "    validate_loader = torch.utils.data.DataLoader(validate_dataset, batch_size=batch_size, shuffle=False, num_workers=nw)\n",
        "    print(\"Using {} images for training, {} images for validation.\".format(train_num, val_num))\n",
        "    lr = 0.0001\n",
        "\n",
        "    epochs = 30\n",
        "    best_acc = 0.0\n",
        "    net = vgg(model_name=model_name, num_classes=2, init_weights=True)\n",
        "    net.to(device)\n",
        "    loss_function = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(net.parameters(), lr=lr)\n",
        "\n",
        "    save_path = '/content/drive/MyDrive/model/{}_7:3.pth'.format(model_name)\n",
        "    train_steps = len(train_loader)\n",
        "\n",
        "    train_losses = []\n",
        "    train_accuracies = []\n",
        "    val_losses = []\n",
        "    val_accuracies = []\n",
        "    val_f1_scores = []\n",
        "    val_precision_scores = []\n",
        "    val_recall_scores = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Train the model\n",
        "        net.train()\n",
        "        running_loss = 0.0\n",
        "        train_correct = 0\n",
        "        train_total = 0\n",
        "        total_f1_score = 0\n",
        "        total_auc = 0\n",
        "        total_accuracy = 0\n",
        "\n",
        "        train_bar = tqdm(train_loader)\n",
        "        for step, data in enumerate(train_bar):\n",
        "            images, labels = data\n",
        "            optimizer.zero_grad()\n",
        "            outputs = net(images.to(device))\n",
        "            loss = loss_function(outputs, labels.to(device))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Calculate the training accuracy\n",
        "            _, predicted = outputs.max(1)\n",
        "            train_total += labels.size(0)\n",
        "            train_correct += predicted.eq(labels.to(device)).sum().item()\n",
        "            train_acc = train_correct / train_total\n",
        "\n",
        "            # Print statistics\n",
        "            running_loss += loss.item()\n",
        "            train_bar.desc = \"Train Epoch [{}/{}] Loss: {:.3f} Acc: {:.3f}\".format(epoch + 1, epochs, loss, train_acc)\n",
        "\n",
        "        # Validate the model\n",
        "        net.eval()\n",
        "        val_loss = 0.0\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "        val_true_labels = []\n",
        "        val_pred_scores = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for val_data in validate_loader:\n",
        "                val_images, val_labels = val_data\n",
        "                outputs = net(val_images.to(device))\n",
        "\n",
        "                # Record the result\n",
        "                val_true_labels.extend(val_labels.numpy())\n",
        "                val_pred_scores.extend(torch.softmax(outputs, dim=1).cpu().numpy()[:, 1])\n",
        "\n",
        "                loss = loss_function(outputs, val_labels.to(device))\n",
        "                val_loss += loss.item()\n",
        "\n",
        "                _, predicted = outputs.max(1)\n",
        "                val_total += val_labels.size(0)\n",
        "                val_correct += predicted.eq(val_labels.to(device)).sum().item()\n",
        "\n",
        "            val_acc = val_correct / val_total\n",
        "            val_loss /= len(validate_loader)\n",
        "\n",
        "        # Calculate the indicators\n",
        "        val_pred_labels = np.array(val_pred_scores) > 0.5\n",
        "        val_f1_score = f1_score(val_true_labels, val_pred_labels)\n",
        "        val_precision_score = precision_score(val_true_labels, val_pred_labels)\n",
        "        val_recall_score = recall_score(val_true_labels, val_pred_labels)\n",
        "\n",
        "        # Record the loss and accuracy\n",
        "        train_losses.append(running_loss / train_steps)\n",
        "        train_accuracies.append(train_acc)\n",
        "        val_losses.append(val_loss)\n",
        "        val_accuracies.append(val_acc)\n",
        "        val_f1_scores.append(val_f1_score)\n",
        "        val_precision_scores.append(val_precision_score)\n",
        "        val_recall_scores.append(val_recall_score)\n",
        "\n",
        "        print('[Epoch %d] Train Loss: %.3f Train Accuracy: %.3f Val Loss: %.3f Val Accuracy: %.3f' %\n",
        "              (epoch + 1, running_loss / train_steps, train_acc, val_loss, val_acc))\n",
        "        print('Val F1 Score: {:.3f}, Precision Score: {:.3f}, Recall Score: {:.3f}'.format(val_f1_score,\n",
        "                                                                                            val_precision_score,\n",
        "                                                                                            val_recall_score))\n",
        "\n",
        "        if val_acc > best_acc:\n",
        "            best_acc = val_acc\n",
        "            torch.save(net.state_dict(), save_path)\n",
        "\n",
        "    fpr, tpr, _ = roc_curve(val_true_labels, val_pred_scores)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    precision, recall, _ = precision_recall_curve(val_true_labels, val_pred_scores)\n",
        "\n",
        "    # Convert a NumPy array to a nested Python list\n",
        "    fpr_lst = fpr.tolist()\n",
        "    tpr_lst = tpr.tolist()\n",
        "    # roc_auc_lst = roc_auc.tolist()\n",
        "    precision_lst = precision.tolist()\n",
        "    recall_lst = recall.tolist()\n",
        "\n",
        "    # Encode a Python list as a string in JSON format\n",
        "    fpr_str = json.dumps(fpr_lst)\n",
        "    tpr_str = json.dumps(tpr_lst)\n",
        "    # roc_auc_str = json.dumps(roc_auc_str)\n",
        "    precision_str = json.dumps(precision_lst)\n",
        "    recall_str = json.dumps(recall_lst)\n",
        "\n",
        "    global result_dict\n",
        "    result_dict = {'train_accuracy': train_accuracies, 'val_accuracy': val_accuracies,\n",
        "                   'val_f1_score': val_f1_scores, 'val_precision_score': val_precision_scores,\n",
        "                   'val_recall_score': val_recall_scores, 'fpr': fpr_str, 'tpr': tpr_str, 'roc_auc': roc_auc,\n",
        "                   'precision': precision_str, 'recall': recall_str}\n",
        "    save_result_path = '/content/drive/MyDrive/model/result_' + model_name + '.json'\n",
        "    with open(save_result_path, 'w') as f:\n",
        "        json.dump(result_dict, f)\n",
        "\n",
        "    print('Finished Training')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vUTLnQUXiPRO"
      },
      "outputs": [],
      "source": [
        "## Tain the model\n",
        "model_name = \"vgg16\"\n",
        "train(model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Discover the unknown unknowns："
      ],
      "metadata": {
        "id": "3_X8REVomvlz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Rxig__dY4vZ8"
      },
      "outputs": [],
      "source": [
        "# Define subclasses of class ImageFolder\n",
        "class CustomImageFolder(datasets.ImageFolder):\n",
        "    def __init__(self, root, transform=None, target_transform=None):\n",
        "        super().__init__(root=root, transform=transform, target_transform=target_transform)\n",
        "\n",
        "# Define class unknownData and blindData\n",
        "class UnknownData:\n",
        "    def __init__(self, image, true_label, predicted_label, confidence, img_tensor,  img_path):\n",
        "        self.image = image\n",
        "        self.true_label = true_label\n",
        "        self.predicted_label = predicted_label\n",
        "        self.confidence = confidence\n",
        "        self.img_tensor = img_tensor.unsqueeze(0)\n",
        "        self.img_path = img_path\n",
        "\n",
        "\n",
        "class BlindData:\n",
        "    def __init__(self, image, true_label, predicted_label, confidence, img_tensor,  img_path):\n",
        "        self.image = image\n",
        "        self.true_label = true_label\n",
        "        self.predicted_label = predicted_label\n",
        "        self.confidence = confidence\n",
        "        self.img_tensor = img_tensor.unsqueeze(0)\n",
        "        self.img_path = img_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rAuSJ4UFZiNU"
      },
      "outputs": [],
      "source": [
        "## Function for evaluating instances\n",
        "\n",
        "def evaluate(model, validate_loader, device, over_confidence, samples, show_wrong=False, show_image=False):\n",
        "    model.eval()\n",
        "    val_correct = 0\n",
        "    val_total = 0\n",
        "    confidences = []\n",
        "\n",
        "    unknown_num = 0\n",
        "    blind_num = 0\n",
        "    unknown_data = []\n",
        "    blind_data = []\n",
        "\n",
        "    sample_indices = random.sample(list(range(len(validate_loader.dataset))), samples)\n",
        "    for i, index in enumerate(sample_indices):\n",
        "        val_images, val_labels = validate_loader.dataset[index]\n",
        "        val_images = val_images.unsqueeze(0).to(device) # add batch dimension\n",
        "\n",
        "        img_np = val_images[0].detach().cpu().permute(1, 2, 0).numpy()\n",
        "        mean = [0.485, 0.456, 0.406]\n",
        "        std = [0.229, 0.224, 0.225]\n",
        "        img_np = (img_np * std) + mean\n",
        "        img_np = np.clip(img_np, 0, 1)\n",
        "        img_np = (img_np * 255.0).astype(np.uint8)\n",
        "        img = Image.fromarray(img_np).convert('RGB')\n",
        "\n",
        "        img_tensor = val_images.squeeze(0).detach().cpu()\n",
        "        val_labels = torch.tensor([val_labels]).to(device)\n",
        "        out = model(val_images)\n",
        "        _, predicted = torch.max(out.data, 1)\n",
        "        confidence = torch.softmax(out, dim=1)[0][predicted[0]].item()\n",
        "\n",
        "        # Determine if an unknown unknown\n",
        "        if predicted[0] != val_labels[0]:\n",
        "            unknown_num += 1\n",
        "            if validate_loader.dataset.classes[val_labels[0]] == 'dress':\n",
        "                img_name = '/content/drive/MyDrive/Deepfashion_data/unknown_data/dress/unknown_{}_true_{}_pred_{}_conf_{:.4f}.jpg'.format(unknown_num,\n",
        "                                  validate_loader.dataset.classes[val_labels[0]],   validate_loader.dataset.classes[predicted[0].item()], confidence)\n",
        "            else:\n",
        "                img_name = '/content/drive/MyDrive/Deepfashion_data/unknown_data/shorts/unknown_{}_true_{}_pred_{}_conf_{:.4f}.jpg'.format(unknown_num,\n",
        "                                  validate_loader.dataset.classes[val_labels[0]],   validate_loader.dataset.classes[predicted[0].item()], confidence)\n",
        "\n",
        "            unknown_data.append(UnknownData(val_images, val_labels, predicted, confidence, img_tensor, img_name))\n",
        "            img.save(img_name)\n",
        "            if confidence >= over_confidence:\n",
        "                blind_num += 1\n",
        "\n",
        "                if validate_loader.dataset.classes[val_labels[0]] == 'dress':\n",
        "                    img_name = '/content/drive/MyDrive/Deepfashion_data/blind_data/dress/blind_{}_true_{}_pred_{}_conf_{:.4f}.jpg'.format(unknown_num,\n",
        "                                      validate_loader.dataset.classes[val_labels[0]],   validate_loader.dataset.classes[predicted[0].item()], confidence)\n",
        "                else:\n",
        "                    img_name = '/content/drive/MyDrive/Deepfashion_data/blind_data/shorts/blind_{}_true_{}_pred_{}_conf_{:.4f}.jpg'.format(unknown_num,\n",
        "                                      validate_loader.dataset.classes[val_labels[0]],   validate_loader.dataset.classes[predicted[0].item()], confidence)\n",
        "\n",
        "                blind_data.append(BlindData(val_images, val_labels, predicted, confidence, img_tensor, img_name))\n",
        "                img.save(img_name)\n",
        "\n",
        "\n",
        "        if show_wrong and predicted[0] == val_labels[0]:\n",
        "            continue\n",
        "\n",
        "        # Show image of unknown unknown\n",
        "        img = None\n",
        "        if show_image:\n",
        "            img_np = val_images[0].detach().cpu().permute(1, 2, 0).numpy()\n",
        "            mean = np.array([0.485, 0.456, 0.406])\n",
        "            std = np.array([0.229, 0.224, 0.225])\n",
        "            img_np = (img_np * std + mean) * 255.0\n",
        "            img_np = np.clip(img_np, 0, 255)\n",
        "            img_np = img_np.astype(np.uint8)\n",
        "            img = Image.fromarray(img_np)\n",
        "            true_label = validate_loader.dataset.classes[val_labels[0]]\n",
        "            pred_label = validate_loader.dataset.classes[predicted[0].item()]\n",
        "            plt.title('True label: {}  \\nPredicted label: {}  \\nConfidence: {:.2f}%'\n",
        "              .format(true_label, pred_label, confidence*100))\n",
        "            plt.imshow(img)\n",
        "            plt.axis('off')\n",
        "            plt.show()\n",
        "\n",
        "        val_correct += predicted.eq(val_labels).sum().item()\n",
        "        val_total += val_labels.size(0)\n",
        "        confidences.append(confidence)\n",
        "\n",
        "    # save unknown data and blind data to files\n",
        "    torch.save([ud.__dict__ for ud in unknown_data], 'unknown_data.pth')\n",
        "    torch.save([bd.__dict__ for bd in blind_data], 'blind_data.pth')\n",
        "\n",
        "    val_acc = val_correct / val_total\n",
        "    return unknown_num, blind_num, blind_data, unknown_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Uf_zc31F9jWi"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "nw = min([os.cpu_count(), batch_size if batch_size > 1 else 0, 8])\n",
        "\n",
        "data_transform = {\n",
        "        \"train\": transforms.Compose([transforms.RandomResizedCrop(224),\n",
        "                                     transforms.RandomHorizontalFlip(),\n",
        "                                     transforms.ToTensor(),\n",
        "                                     transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]),\n",
        "        \"val\": transforms.Compose([transforms.Resize((224, 224)),\n",
        "                                   transforms.ToTensor(),\n",
        "                                   transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])}\n",
        "\n",
        "image_path = '/content/drive/MyDrive/Deepfashion_data/outdata'\n",
        "\n",
        "# Using the defined subclass CustomImageFolder, the dataset attribute is needed in evaluate function.\n",
        "validate_dataset = CustomImageFolder(root=os.path.join(image_path, \"val\"),\n",
        "                                       transform=data_transform[\"val\"])\n",
        "validate_loader = torch.utils.data.DataLoader(validate_dataset, batch_size=batch_size, shuffle=False, num_workers=nw)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "peXoRdEkgoNf"
      },
      "outputs": [],
      "source": [
        "# Same parameters and network structure as the previously trained model\n",
        "model = vgg(model_name=\"vgg16\", num_classes=2, init_weights=True)\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "#model.eval()\n",
        "\n",
        "# Load the trained model\n",
        "saved_model_path = \"/content/drive/MyDrive/model/vgg16_7:3.pth\"\n",
        "model.load_state_dict(torch.load(saved_model_path, map_location=device))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "k6vLUJ9gfpIr"
      },
      "outputs": [],
      "source": [
        "## Display unknown unknowns information\n",
        "\n",
        "unknown_num, blind_num, blind_data, unknown_data = evaluate(model, validate_loader, device, over_confidence=0.65, samples=800, show_wrong=True, show_image=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Interprete the unknown unknowns with CAM:"
      ],
      "metadata": {
        "id": "sXPN3KHSnbI2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6amckKl4pPQF"
      },
      "outputs": [],
      "source": [
        "from google.colab.patches import cv2_imshow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TG6DnFVRM5RL"
      },
      "outputs": [],
      "source": [
        "## CAM method\n",
        "def draw_CAM(model, img_dir, save_dir, transform=None, visual_heatmap=False):\n",
        "\n",
        "    for classes in os.listdir(img_dir):\n",
        "        img_folder_path = os.path.join(img_dir, classes)\n",
        "        save_folder_path = os.path.join(save_dir, classes)\n",
        "        for i in os.listdir(img_folder_path):\n",
        "            img_path = os.path.join(img_folder_path, i)\n",
        "            print(img_path)\n",
        "            save_path = os.path.join(save_folder_path, i)\n",
        "            # save_path = \"CAM_\" + save_path\n",
        "            print(save_path)\n",
        "\n",
        "            # Image loading & pre-processing\n",
        "            img = Image.open(img_path).convert('RGB')\n",
        "            if transform:\n",
        "                img = transform(img)\n",
        "            else:\n",
        "                transform = transforms.Compose([\n",
        "                    transforms.Resize((224, 224)),\n",
        "                    transforms.ToTensor(),\n",
        "                    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "                ])\n",
        "                img = transform(img)\n",
        "\n",
        "            device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "            img = img.unsqueeze(0).to(device)\n",
        "\n",
        "\n",
        "            # features is the feature map output by conv13\n",
        "            model.eval()\n",
        "            features = model.features(img)\n",
        "            x = features.view(features.size(0), -1)\n",
        "            output = model.classifier(x)\n",
        "\n",
        "            # Auxiliary function to read the intermediate gradient\n",
        "            def extract(g):\n",
        "                global features_grad\n",
        "                features_grad = g\n",
        "\n",
        "            # The output of the highest scoring category\n",
        "            pred = torch.argmax(output).item()\n",
        "            pred_class = output[:, pred]\n",
        "\n",
        "            # Get the gradient\n",
        "            features.register_hook(extract)\n",
        "            pred_class.backward()\n",
        "            grads = features_grad\n",
        "\n",
        "            # Adaptive average pooling\n",
        "            pooled_grads = torch.nn.functional.adaptive_avg_pool2d(grads, (1, 1))\n",
        "\n",
        "            # Remove batch_size\n",
        "            pooled_grads = pooled_grads[0]\n",
        "            features = features[0]\n",
        "\n",
        "            # 512 is the number of channels in the last layer of the feature\n",
        "            # Matrix multiplication\n",
        "            for i in range(512):\n",
        "                features[i, ...] *= pooled_grads[i, ...]\n",
        "                # features[0] *= pooled_grads[0]\n",
        "                # features[1] *= pooled_grads[1]\n",
        "                # ……\n",
        "                # features[511] *= pooled_grads[511]\n",
        "\n",
        "            heatmap = features.cpu().detach()\n",
        "            heatmap = np.mean(heatmap.numpy(), axis=0)\n",
        "\n",
        "            # Positive pixels are retained, negative values are set to 0\n",
        "            heatmap = np.maximum(heatmap, 0)\n",
        "            heatmap /= np.max(heatmap)\n",
        "\n",
        "            # if visual_heatmap:\n",
        "            #     plt.matshow(heatmap)\n",
        "            #     plt.show()\n",
        "\n",
        "            img = cv2.imread(img_path)  # load original image\n",
        "            print(img.shape[1])\n",
        "\n",
        "            # Resize the heat map to the same size as the original image\n",
        "            heatmap_SIZE = cv2.resize(heatmap, (img.shape[1], img.shape[0]))\n",
        "            # Convert heat maps to RGB format\n",
        "            heatmap_R = np.uint8(255 * heatmap_SIZE)\n",
        "\n",
        "            # Apply heat maps to raw images\n",
        "            heatmap_RGB = cv2.applyColorMap(heatmap_R, cv2.COLORMAP_JET)\n",
        "            # Convert cv plots to plt plots, otherwise the thermal part will turn blue\n",
        "            heatmap_RGB = cv2.cvtColor(heatmap_RGB, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "            alpha = 0.4  # heat map transparency parameter\n",
        "            x, y, z = np.where(heatmap_RGB > 0)\n",
        "            heatmap_only = np.zeros_like(img)\n",
        "            heatmap_only[x, y, :] = heatmap_RGB[x, y, :]\n",
        "\n",
        "            # Fuse images to increase contrast and saturation\n",
        "            superimposed_img = heatmap_only * alpha + img * (1 - alpha)\n",
        "            superimposed_img = superimposed_img.astype(np.uint8)\n",
        "\n",
        "            fig, axs = plt.subplots(2, 2, figsize=(10, 10))\n",
        "\n",
        "            img = Image.open(img_path).convert('RGB')\n",
        "\n",
        "            axs[0, 0].imshow(heatmap)\n",
        "            axs[0, 0].set_title('(a)heatmap')\n",
        "\n",
        "            axs[0, 1].imshow(heatmap_RGB)\n",
        "            axs[0, 1].set_title('(b)RGB heatmap')\n",
        "\n",
        "            axs[1, 0].imshow(img)\n",
        "            axs[1, 0].set_title('(c)image')\n",
        "\n",
        "            axs[1, 1].imshow(superimposed_img)\n",
        "            axs[1, 1].set_title('(d)image with CAM')\n",
        "\n",
        "\n",
        "            plt.rcParams['font.size'] = 14\n",
        "\n",
        "            plt.tight_layout()\n",
        "\n",
        "            plt.show()\n",
        "\n",
        "            plt.savefig(save_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N5XGpAnvbAZW"
      },
      "outputs": [],
      "source": [
        "save_dir = '/content/drive/MyDrive/Deepfashion_data/test_data'\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "img_dir = '/content/drive/MyDrive/Deepfashion_data/test_data'\n",
        "draw_CAM(model, img_dir, save_dir, transform=None, visual_heatmap=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vPhHcKHVDe26"
      },
      "source": [
        "Interprete the unknown unknowns with LIME:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lZPKQ-9hJAsS"
      },
      "outputs": [],
      "source": [
        "!pip install -i https://pypi.tuna.tsinghua.edu.cn/simple lime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P8SQN3XnI8QH"
      },
      "outputs": [],
      "source": [
        "import lime\n",
        "from lime import lime_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lOApbCV9Ua_o"
      },
      "outputs": [],
      "source": [
        "from skimage.segmentation import mark_boundaries\n",
        "import skimage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VqcJdgRN7Wxi"
      },
      "outputs": [],
      "source": [
        "## Define the classifier function predict\n",
        "\n",
        "def predict(images):\n",
        "    input_tensors = []\n",
        "    for image in images:\n",
        "        # Perform image preprocessing and convert it to PyTorch tensor\n",
        "        preprocess = transforms.Compose([\n",
        "            transforms.Resize(256),\n",
        "            transforms.CenterCrop(224),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "        input_tensor = preprocess(Image.fromarray(image)).unsqueeze(0)\n",
        "        input_tensors.append(input_tensor)\n",
        "\n",
        "    # Put the input tensor into the model for forward propagation\n",
        "    with torch.no_grad():\n",
        "        outputs = [model(input_tensor) for input_tensor in input_tensors]\n",
        "\n",
        "    # Calculate the prediction (return the output vector without taking the one with the highest probability)\n",
        "    predicted_labels = torch.cat(outputs, dim=0)\n",
        "\n",
        "    return predicted_labels.numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ja-q_Jma7ZTv"
      },
      "outputs": [],
      "source": [
        "# LIME method\n",
        "\n",
        "def limeToshow(image_list, topK, lime_save_dic, num):\n",
        "\n",
        "    global explanation_list\n",
        "    explanation_array = []\n",
        "    i = num\n",
        "    for image_path in image_list:\n",
        "        # Load an image file and convert it to a Numpy array\n",
        "        i += 1\n",
        "        print(image_path)\n",
        "        lime_save_path = lime_save_dic + str(i)\n",
        "        print(lime_save_path)\n",
        "\n",
        "        pil_img = Image.open(image_path).convert('RGB')\n",
        "        img_array = np.array(pil_img)\n",
        "        img_array = np.expand_dims(img_array, axis=0)\n",
        "\n",
        "        # Create an interpreter object\n",
        "        explainer = lime_image.LimeImageExplainer()\n",
        "        explanation = explainer.explain_instance(img_array[0], classifier_fn=predict, top_labels=2)\n",
        "        explanation_list.append(explanation)\n",
        "\n",
        "        # visualization\n",
        "        temp, mask = explanation.get_image_and_mask(explanation.top_labels[0], positive_only=True, num_features=topK, hide_rest=False)\n",
        "        temp = (temp - temp.min()) / (temp.max() - temp.min())\n",
        "\n",
        "        plt.imshow(mark_boundaries(temp, mask))\n",
        "        plt.savefig(lime_save_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H4kW4_-xGppF"
      },
      "outputs": [],
      "source": [
        "image_list = []\n",
        "\n",
        "dir_path = '/content/drive/MyDrive/Deepfashion_data/blind_data/'\n",
        "\n",
        "for file in os.listdir(dir_path):\n",
        "    if os.path.isfile(os.path.join(dir_path, file)):\n",
        "        image_list.append(file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VU5nqjHYRwXX"
      },
      "outputs": [],
      "source": [
        "topK = 2\n",
        "lime_save_dic = '/content/drive/MyDrive/Deepfashion_data/lime_data/'\n",
        "\n",
        "limeToshow(image_list, topK, lime_save_dic, len(image_list))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}